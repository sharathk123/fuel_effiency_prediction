{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d98f67a0",
   "metadata": {},
   "source": [
    "## Step 1: Read Data and Rename Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e112747b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Interactive file picker if file not found\n",
    "import os\n",
    "if not os.path.exists(\"data/Automobile_data.csv\"):\n",
    "    import tkinter as tk\n",
    "    from tkinter import filedialog\n",
    "    root = tk.Tk()\n",
    "    root.withdraw()\n",
    "    file_path = filedialog.askopenfilename(title=\"Select Automobile_data.csv\", filetypes=[(\"CSV files\", \"*.csv\")])\n",
    "    if file_path:\n",
    "        df = pd.read_csv(file_path)\n",
    "    else:\n",
    "        raise FileNotFoundError(\"No file selected. Please select a valid CSV file.\")\n",
    "else:\n",
    "    df = pd.read_csv(\"data/Automobile_data.csv\")\n",
    "\n",
    "df.rename(columns={\n",
    "    'r': 'range_km',\n",
    "    'm (kg)': 'mass_kg',\n",
    "    'Mt': 'co2_emission_tons',\n",
    "    'Ewltp (g/km)': 'co2_wltp_g_per_km',\n",
    "    'Ft': 'fuel_type',\n",
    "    'Fm': 'fuel_mix',\n",
    "    'ec (cm3)': 'engine_capacity_cc',\n",
    "    'ep (KW)': 'engine_power_kw',\n",
    "    'z (Wh/km)': 'energy_consumption_whpkm',\n",
    "    'Erwltp (g/km)': 'co2_reduction_wltp_gpkm',\n",
    "    'Fuel consumption': 'fuel_consumption',\n",
    "    'Electric range (km)': 'electric_range_km'\n",
    "}, inplace=True)\n",
    "\n",
    "df.head()\n",
    "print(df.columns.tolist())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b74bb3b6",
   "metadata": {},
   "source": [
    "## Step 2: Handle Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "31e1ab28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill numerical columns with median\n",
    "num_cols = df.select_dtypes(include='number').columns\n",
    "df[num_cols] = df[num_cols].fillna(df[num_cols].median())\n",
    "\n",
    "# Fill categorical columns with mode\n",
    "cat_cols = df.select_dtypes(include='object').columns\n",
    "df[cat_cols] = df[cat_cols].fillna(df[cat_cols].mode().iloc[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91af384c",
   "metadata": {},
   "source": [
    "## Step 3: Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bb052184",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example feature: power to weight ratio\n",
    "df['power_to_weight'] = df['engine_power_kw'] / df['mass_kg']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5792fd9a",
   "metadata": {},
   "source": [
    "## Step 4: Outlier Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "efcd2927",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "df.columns = df.columns.str.strip()\n",
    "def cap_outliers(df, cols):\n",
    "    for col in cols:\n",
    "        Q1 = df[col].quantile(0.25)\n",
    "        Q3 = df[col].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        lower_bound = Q1 - 1.5 * IQR\n",
    "        upper_bound = Q3 + 1.5 * IQR\n",
    "        df[col] = df[col].clip(lower_bound, upper_bound)\n",
    "    return df\n",
    "\n",
    "cap_columns = ['Fuel consumption', 'mass_kg', 'engine_power_kw', 'co2_wltp_g_per_km']\n",
    "df = cap_outliers(df, cap_columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "88fa218c-741c-4a82-b830-b7adb1377d6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing Values After Imputation:\n",
      " Series([], dtype: int64)\n"
     ]
    }
   ],
   "source": [
    "# Check if any missing values remain\n",
    "missing_summary = df.isnull().sum()\n",
    "print(\"Missing Values After Imputation:\\n\", missing_summary[missing_summary > 0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e73d5cc1-a99c-4547-977c-f82adb93c7cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "if df['range_km'].nunique() == 1:\n",
    "    df.drop(columns=['range_km'], inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8552f8a1-a35d-47e6-b31a-f6c7afa14763",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fuel consumption: 0 outliers remaining\n",
      "IQR bounds: (4.30, 6.70)\n",
      "\n",
      "mass_kg: 0 outliers remaining\n",
      "IQR bounds: (527.50, 2507.50)\n",
      "\n",
      "engine_power_kw: 0 outliers remaining\n",
      "IQR bounds: (-13.00, 219.00)\n",
      "\n",
      "co2_wltp_g_per_km: 0 outliers remaining\n",
      "IQR bounds: (44.00, 196.00)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def iqr_outlier_summary(data, columns):\n",
    "    for col in columns:\n",
    "        Q1 = data[col].quantile(0.25)\n",
    "        Q3 = data[col].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        lower_bound = Q1 - 1.5 * IQR\n",
    "        upper_bound = Q3 + 1.5 * IQR\n",
    "        outliers = data[(data[col] < lower_bound) | (data[col] > upper_bound)]\n",
    "        print(f\"{col}: {len(outliers)} outliers remaining\")\n",
    "        print(f\"IQR bounds: ({lower_bound:.2f}, {upper_bound:.2f})\\n\")\n",
    "\n",
    "# List of numerical columns to check\n",
    "iqr_columns = ['Fuel consumption', 'mass_kg', 'engine_power_kw', 'co2_wltp_g_per_km']\n",
    "iqr_outlier_summary(df, iqr_columns)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc491a0f",
   "metadata": {},
   "source": [
    "## Step 5: Encoding and Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5371a55e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Separate features and target\n",
    "X = df.drop(columns='Fuel consumption')\n",
    "y = df['Fuel consumption']\n",
    "\n",
    "# Identify columns\n",
    "num_features = X.select_dtypes(include='number').columns.tolist()\n",
    "cat_features = X.select_dtypes(include='object').columns.tolist()\n",
    "\n",
    "# Preprocessor\n",
    "preprocessor = ColumnTransformer([\n",
    "    (\"num\", StandardScaler(), num_features),\n",
    "    (\"cat\", OneHotEncoder(handle_unknown='ignore'), cat_features)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8ba0975",
   "metadata": {},
   "source": [
    "## Step 6: PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea520186",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply PCA after preprocessing\n",
    "pca_pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('pca', PCA(n_components=0.95))\n",
    "])\n",
    "\n",
    "X_pca = pca_pipeline.fit_transform(X)\n",
    "print(\"Original shape:\", X.shape)\n",
    "print(\"Reduced shape:\", X_pca.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "164b101a",
   "metadata": {},
   "source": [
    "## Step 7: Model Building and Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc2c98b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, HistGradientBoostingRegressor\n",
    "from sklearn.model_selection import RandomizedSearchCV, train_test_split\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from xgboost import XGBRegressor\n",
    "import numpy as np\n",
    "\n",
    "# Split data (700k train, 200k validation, rest test)\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, train_size=700000, random_state=42)\n",
    "X_valid, X_test, y_valid, y_test = train_test_split(X_temp, y_temp, test_size=0.33, random_state=42)\n",
    "\n",
    "# Sample for tuning\n",
    "X_sample = X_train.sample(n=100000, random_state=42)\n",
    "y_sample = y_train.loc[X_sample.index]\n",
    "\n",
    "def tune_model(name, model, params):\n",
    "    pipe = Pipeline([\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('model', model)\n",
    "    ])\n",
    "    search = RandomizedSearchCV(\n",
    "        pipe,\n",
    "        param_distributions=params,\n",
    "        n_iter=10,\n",
    "        cv=3,\n",
    "        scoring='r2',\n",
    "        verbose=1,\n",
    "        n_jobs=-1,\n",
    "        random_state=42\n",
    "    )\n",
    "    search.fit(X_sample, y_sample)\n",
    "    print(f\"{name} best RÂ² score: {search.best_score_:.4f}\")\n",
    "    return search\n",
    "\n",
    "# Parameter grids\n",
    "param_rf = {\n",
    "    'model__n_estimators': [100, 200],\n",
    "    'model__max_depth': [10, None],\n",
    "    'model__min_samples_split': [2, 5]\n",
    "}\n",
    "param_gbr = {\n",
    "    'model__n_estimators': [100, 200],\n",
    "    'model__learning_rate': [0.05, 0.1],\n",
    "    'model__max_depth': [3, 5]\n",
    "}\n",
    "param_hgb = {\n",
    "    'model__max_iter': [100, 200],\n",
    "    'model__learning_rate': [0.05, 0.1]\n",
    "}\n",
    "param_xgb = {\n",
    "    'model__n_estimators': [100, 200],\n",
    "    'model__max_depth': [3, 5],\n",
    "    'model__learning_rate': [0.05, 0.1],\n",
    "    'model__subsample': [0.8, 1.0],\n",
    "    'model__colsample_bytree': [0.8, 1.0]\n",
    "}\n",
    "\n",
    "# Model tuning\n",
    "search_rf = tune_model(\"RandomForest\", RandomForestRegressor(), param_rf)\n",
    "search_gbr = tune_model(\"GradientBoosting\", GradientBoostingRegressor(), param_gbr)\n",
    "search_hgb = tune_model(\"HistGradientBoosting\", HistGradientBoostingRegressor(), param_hgb)\n",
    "search_xgb = tune_model(\"XGBoost\", XGBRegressor(objective='reg:squarederror', random_state=42), param_xgb)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ca8ec74",
   "metadata": {},
   "source": [
    "## Step 8: Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4964f0e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(name, model, X_data, y_data, dataset_name):\n",
    "    y_pred = model.predict(X_data)\n",
    "    rmse = np.sqrt(mean_squared_error(y_data, y_pred))\n",
    "    mae = mean_absolute_error(y_data, y_pred)\n",
    "    r2 = r2_score(y_data, y_pred)\n",
    "    print(f\"{name} on {dataset_name} set: RMSE={rmse:.2f}, MAE={mae:.2f}, R2={r2:.4f}\")\n",
    "\n",
    "models = {\n",
    "    \"RandomForest\": search_rf,\n",
    "    \"GradientBoosting\": search_gbr,\n",
    "    \"HistGradientBoosting\": search_hgb,\n",
    "    \"XGBoost\": search_xgb\n",
    "}\n",
    "\n",
    "for model_name, model in models.items():\n",
    "    evaluate_model(model_name, model, X_train, y_train, \"Train\")\n",
    "    evaluate_model(model_name, model, X_valid, y_valid, \"Validation\")\n",
    "    evaluate_model(model_name, model, X_test, y_test, \"Test (Production)\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fuel_efficiency_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
